## Decision Tree : 개요

#### 목적
- 한 번에 하나씩의 설명변수를 사용하여 정확한 예측이 가능한 규칙들의 집합을 생성
- 최종 결과물은 나무를 뒤집어 놓은 형태인 규칙들의 집합

#### 용어
- 노드(Node) : 입력 데이터 공간의 특정 영역
- 부모 노드(Parent node) : 분기(split) 전 노드
- 자식 노드(Child node) : 부모 노드로부터 분기 후 파생된 노드
- 분기 기준(Split criterion) : 한 부모 노드를 두 개 이상의 자식 노드들로 분기하는데 사용되는 변수 및 기준값
- 시작/뿌리 노드(Root node) : 전체 데이터를 포함하는 노드. 자식 노드만 존재하며 부모 노드 존재하지 않음
- 말단/잎새 노드(Leaf node) : 더 이상 분기가 수행되지 않는 노드. 부모 노드만 존재하며 자식 노드는 존재하지 않음
![JPG](/assets/images/트리 구조.JPG)

#### 장점
- **결과를 사람이 이해할 수 있는 규칙의 형태로 제공한다.**
    - if-then 형식으로 표현되는 규칙을 생성함으로써 **결과에 대한 예측과 함께 그 이유를 설명할 수 있다.**
- 데이터의 사전 전처리를 최소화함(정규화/결측치 저리 등을 하지 않아도 됨)
- 수치형 변수와 범주형 변수를 함께 다룰 수 있다.

#### 핵심 아이디어
- **재귀적 분기 : Recursive Partitioning**
    - 입력변수의 영역을 두 개로 구분 -> 구분하기 전보다 구분된 뒤에 **각 영역의 순도(purity, homogeneity)가 증가**하도록
    - 하나의 영역에 몰려있도록 하는 분기점 찾기
- **가지치기 : Pruning the Tree**
    - **과적합을 방지**하기 위해 너무 자세하게 구분된 영역을 통합
- 재귀적 분기 -> 가지치기(사후적 가지치기)

### CART : Classification and Regression
- 분류와 회귀 모두 가능한 DT
- 개별 변수의 영역을 반복적으로 분할함으로써 전체 영역에서의 규칙을 생성하는 지도학습 기법이다.

#### 재귀적 분기
- 특정 영역(부모 노드)에 속하는 개체들을 <u>하나의 기준 변수 값</u>의 범위에 따라 분기한다.
- 분기에 의해 새로 생성된 자식 노드의 <u>동질성이 최대화</u> 되도록 분기점을 선택한다.
- 불순도를 측정하는 기준(범주형 변수에서는 <u>지니계수</u>, 수치형 변수에서는 <u>분산</u>)을 이용한다.

#### 가지치기
- 과적합을 방지하기 위해 하위 노드들을 상위 노드로 결합하는 것이다.
- 사전적 가지치기(Pre-pruning) : Tree를 생성하는 과정에서 최소 분기 기준을 이용
- 사후적 가지치기(Post-pruning) : Full-tree 생성 후 검증 데이터의 오분류율과 Tree의 복잡도(말단 노드의 수) 등을 고려

### 재귀적 분기
- 예시 : 잔디깎기 기계 구입 예측
    - 범주형 변수
    - 24개의 가정에 대해 잔디깎기 기계 구입 여부를 분류
    - 설명 변수 : 수입(Income), 집의 크기(Lot size)

- 전체 영역을 어떻게 분할해야 할까?
    - 각 노드에 한 범주에 속하는 객체들이 다수 존재하도록 분할하는 것이 좋다. => 동질성 최대화
    - 이러한 개념을 하나의 숫자로 정량화 : 불순도 지표

### 불순도 지표 1 : 지니 계수(Gini Index)
- 불순도 지표 : 특정 영역에 개별적인 객체들이 얼마나 혼재되어 있는가를 평가하는 지표
- 이 지표를 사용해서 어떠한 선을 찾는데,
    - 이 선은 반드시 불순도가 가장 낮아지는(=정보획득이 커지는) 선이어야 한다.

#### 하나의 영역에 대한 지니 계수 (분할 전 지니계수)
- C개의 범주가 존재하는 A 영역에 대한 지니 계수 
![JPG](/assets/images/한 영역에 대한 지니계수.JPG)

- 두 범주만 존재하는 상황에서 영역 A의 모든 객체들이 동일한 범주에 속할 경우 : I(A) = 0
    - $1-(\frac{1}{1})^2 - (\frac{0}{1})^2 = 1-1-0 = 0$
- 두 범주만 존재하는 상황에서 영역 A에 속하는 객체들이 1/2의 비중으로 존재할 경우 : I(A) = 0.5 => 혼잡도가 가장 높은 경우
    - $1-(\frac{1}{2})^2 - (\frac{1}{2})^2 = 1-\frac{1}{4}-\frac{1}{4} = 0.5$
    
#### 두 개 이상의 영역에 대한 지니 계수 (분할 후 지니계수) 
![JPG](/assets/images/두 개 이상의 영역에 대한 지니계수.JPG)

- 분기 후의 **정보획득(Information gain)** : 0.4688 - 0.3438 = **0.1250** => 분기 기준(빨잔 점선)의 효과
    - 0.4688 : 분기 전 A영역에 대한 지니계수
    - 0.3438 : A영역 분기 후 두 영역(또는 이상)에 대한 지니계수
    
#### Q. 어떤 분기 기준이 좋아요? 
#### A. 정보획득이 더 큰 분기기준이요!

### 불순도 지표 2 : Deviance


### 재귀적 분기 : 절차

#### 범주형 변수 : 분기가 가능한 모든 경우의 수를 조사하여 각각의 정보 획득을 계산하고, 가장 최적값을 사용한다.

1. 한 변수를 기준으로 정렬한다.
    - Lot size 변수
    - 이때 변수의 정렬 순서나 기준(오름차순/내림차순)은 최종 모델에 영향을 미치지 않는다. (모든 경우의 수를 다 해보기 때문에)


2. 순차적으로 가능한 분기점에 대한 정보획득을 계산한다.
    - 첫 번째 후보 분기점 : 14.4(0.5*(14.0+14.8))
        - 첫 번째 후보 분기점 ~ 마지막 후보 분기점까지의 정보회득을 계산한다.
    - Lot size < 14.4와 Lot size>14.4인 두 영역으로 구분
    - 불순도(impurity) 계산 : 지니계수
    - 분기 전 : $1-(\frac{12}{24})^2 - (\frac{12}{24})^2 = 0.5000$ => 최악의 상황!(반반)
    - 분기 후 : $\frac{1}{24} x (1-(\frac{0}{1})^2 - (\frac{1}{1})^2) + \frac{23}{24} x (1-(\frac{12}{23})^2 - (\frac{11}{23})^2) = 0.4783$
    - 정보 획득 : $0.5000 - 0.4783 = 0.0217$
![JPG](/assets/images/첫번째 후보 분기점 표.JPG)
<center> [Lot size 정렬 표] </center>

![JPG](/assets/images/첫번째 후보 분기점 그래프.JPG)
<center> [Lot size = 14.4 기준 분기 그래프] </center>
    
    
3. 또 다른 변수를 기준으로 정렬하여 위 과정을 반복한다.(모든 변수 사용)
    - Income 변수를 기준으로 다시 정렬한다.
    - 위와 같이 순차적으로 가능한 분기점에 대한 정보획득을 계산한다.


4. 최적의 분기점을 찾아서 첫 번째 분기점으로써 분할한다.
    - 최적 분기점 : Lot size 19.0
    - 정보획득 : $0.5000 - 0.3750 = 0.1250$ $\Rightarrow$ 가장 큰 값!
![JPG](/assets/images/첫번째 분기점 그래프.JPG)


5. **모든 노드의 순도가 100%가 될 때까지 분기를 수행한다.** 
    - 정보획득이 0이 되는 시점까지 수행
    - ★한 번의 분기로 끝나는 것이 아니라 여기서부터 다시 시작해서 똑같은 작업을 수행한다.=> **재귀적**
    - 두 번째 분기점 : Income 84.75
![JPG](/assets/images/두번째 분기점 총.JPG)


6. 재귀적 분기 완료
    - 모든 말단 노드에는 한 범주의 객체들만 존재한다.
    - Full Tree 완성 -> 순도 100% => Training Data Error = 0 (완벽 구분)
![JPG](/assets/images/재귀적 분기 완료 그래프.JPG)
