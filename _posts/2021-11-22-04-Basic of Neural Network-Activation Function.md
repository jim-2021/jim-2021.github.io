## 활성함수 (Activation Function)

딥러닝의 네트워크 구조에서는 노드에 들어온 값들을 바로 다음 레이어에 전달하지 하지 않고, 주로 비선형(Non-linear) 함수를 통과시킨 후에 전달한다. 이때의 함수를 활성함수(Activation Function)라고 한다.

![JPG](/assets/images/activation function.JPG)

#### 활성함수는 퍼셉트론의 출력에 의미를 부여해준다.
- 노드에 들어온 **입력값이 조건을 만족하면 활성화**가 되고, **그렇지 않으면 비활성화**가 되도록 하기 때문이다.
- 예를 들어 ReLU 함수의 경우, 입력값이 음수이면 0을 출력하고, 0 이상이면 입력값을 그대로 출력한다.

#### 이때 활성화가 되는 기준을 먼저 정해야 한다. 
- 만약 출력값이 0보다 큰 경우를 활성화되었다고 정한다면, ReLU 함수의 경우 음수일 때는 비활성화되고, 0 이상인 경우에 활성화가 되는 함수가 된다.

#### 활성함수를 사용하는 이유는? (비선형 함수를 사용하는 이유)
- 선형함수는 노드의 개수가 아무리 많아도 결국 하나의 노드를 사용하는 것과 차이가 없다. 즉 은닉계층이 없는 네트워크로 표현되기 때문에 뉴럴 네트워크에서 층을 쌓는 혜택을 얻을 수 없다. $\rightarrow$ 모델의 표현력 떨어짐
- 선형함수로는 비선형적 특성을 지닌 데이터를 예측하지 못한다.
- 딥러닝에서 주로 사용하는 복잡한 특징을 가진 **비선형 데이터를 표현하기 위해서는 딥러닝 모델도 비선형성을 지니고 있어야 한다.**
    - 비선형 활성함수를 layer 사이 사이에 널어주면 모델이 비선형 데이터도 표현할 수 있게 된다.
- 이렇게 활성함수를 사용함으로써 **모델의 표현력을 향상**시킬 수 있다.


- 역전파 알고리즘을 사용할 수 있다.
- 다중 출력이 가능하다.(다중 분류 문제 가능)
- 비선형적인 특성을 지닌 데이터의 예측이 가능하다.

#### sign 함수 (계단 함수)
- 초기 퍼셉트론의 활성함수로 사용되었다.
- 이진 분류 문제에서 유용하다.
- 선형적으로 구분 가능한 문제를 구현할 수 있다.(AND, OR gate)

**한계**

- 직선(결정 경계)의 우측, 좌측에 위치한다는 정보만 가지고 판단한다. 즉, 직선으로부터의 거리를 신경쓰지 않는다.
- 역전파 알고리즘을 사용하지 못한다.
- 현실의 복잡한 문제를 해결하기 어렵다.
- 다중 출력이 불가능하다. $\rightarrow$ 다양한 클래스를 구분하는 문제 해결 불가능

딥러닝에서 사용되는 활성함수에 대해 알아보자.

### Sigmoid Function

![JPG](/assets/images/sigmoid function 수식.JPG)
![JPG](/assets/images/sigmoid function graph.JPG)

- 값이 작아질수록 0, 커질수록 1에 수렵한다.
- 출력값이 0~1 사이로, 확률을 표현할 수 있다.
- 모든 실수 입력값에 대해 출력이 정의된다.
- 모든 점에서 미분이 가능하다.
- Sigmoid 함수는 원래 'S' shape의 모든 함수를 가르키나. 딥러닝에서는 이러한 함수로 통용된다.
- 이진 분류에 사용

### Tanh(Hyperbolic tangent) Function

![JPG](/assets/images/tanh function 수식.JPG)
![JPG](/assets/images/tanh function graph.JPG)

- 값이 작아질수록 -1, 커질수록 1에 수렴한다.
- 모든 실수 입력값에 대해 출력이 정의된다.
- 출력이 실수값으로, Soft decision
- 모든 점에서 미분이 가능하다.
- 함수의 중심값을 0으로 옮겨 sigmoid의 최적화 과정이 느려지는 문제를 해결했다.

### Softmax Function

![JPG](/assets/images/softmax 수식.JPG)

- 각 입력의 지수함수를 정규화한 것이다.
    - 지수함수 : 모든 입력이 0보다 크게 된다.
    - 정규화 : 전부 합쳐서 1이 되도록 한다.
- 각 출력은 0~1 사이의 값을 가지고, 모든 출력의 합은 반드시 1이 된다.
- 여러 경우의 수 중 한 가지에 속할 '확률'을 표현한다.
- 다중 분류에 사용

### ReLU Function

![JPG](/assets/images/relu function 수식.JPG)
![JPG](/assets/images/relu function graph.JPG)

- 입력이 양수일 때는 그 값을 그대로, 음수일 때는 0을 출력한다.
- 딥러닝에서 가장 많이 사용되는 활성함수이다.
- 미분 값이 일정해서(0 또는 1) 학습이 잘 된다.
- 단순한 구현으로 빠른 연산이 가능하다.
